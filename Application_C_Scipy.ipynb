{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciPy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Généralités\n",
    "\n",
    "Vous l'aurez peut-être remarqué, si vous êtes un afficionado de MATLAB : certaines fonctions de votre programme préféré _manquent à l'appel !_. Par exemple, nous n'avons pas vu comment résoudre numériquement une équation différentielle, comment ajuster (_fitter_) des données autrement que par une droite, comment filtrer un signal, chercher les maxima d'une fonction ou ses zéros, ... voire même lire un fichier `.mat`de MATLAB ! C'est le but de ce module `Scipy`.\n",
    "\n",
    "Scipy s'appuie principalement sur NumPy, donc je vous conseille d'avoir jeté un oeil au [tutoriel NumPy](./Application_A_Numpy.ipynb) et d'avoir les bases de [Matplotlib](./Application_B_Matplotlib.ipynb) afin de ne pas être trop perdu.e. Je traiterai dans l'ordre :\n",
    "\n",
    "- [le filtrage](#Filtrage) pour supprimer le bruit ou les composantes bizarroïdes de vos données expérimentales.\n",
    "- [l'interpolation](#Interpolation) afin d'obtenir des données échantillonnées régulièrement. J'évoquerai également la possibilité d'utiliser l'interpolation pour atteindre une précision _sub-pixel_.\n",
    "- [la recherche de maxima, minima et zéros](#Maxima,-Minima-et-Zeros) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "### Filtrage\n",
    "\n",
    "Choisissons un signal $e(t)$ à temps _continu_. Filtrer ce signal revient à _convoluer_ celui-ci par une fonction de réponse $h(t)$: \n",
    "\n",
    "$$ s(t) = (h * e)(t) =  \\int_{-\\infty}^{\\infty} e(\\tau) h(t - \\tau) {\\rm d}\\tau$$\n",
    "\n",
    "Cette fonction $h$ peut être pensée pour dériver, intégrer, multiplier, etc. la fonction initiale $s(t)$. \n",
    "\n",
    "Dans cette section, en fonction de vos usages, nous allons voir comment filtrer des signaux : \n",
    "\n",
    "* En utilisant des filtres basés sur des _fonctions de transfert_, c'est à dire en travaillant dans l'espace de Fourier.\n",
    "* En construisant explicitement des _fenêtres_ de filtrage $h(t)$. \n",
    "* En utilisant l'algorithme de _Savitsky-Golay_."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonctions de transfert\n",
    " \n",
    "Dans cette section, nous allons apprendre à définir des fonctions de transfert 'à l'ancienne', c'est à dire dans l'espace de Fourier, puis de voir comment les 'convertir' en filtres numériques afin de les appliquer à des signaux échantillonnés régulièrement. Je vous montrerai enfin comment directement définir des filtres numériques qui se rapprochent de ces comportements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quelques rappels \n",
    "\n",
    "Les fonctions de transfert $H(j\\omega)$ sont simplement les transformées de Fourier des fenêtres $h(t)$ que nous avons évoquées ci-dessus. Travailler dans l'espace de Fourier est très pratique car la convolution $*$ devient alors une simple multiplication $\\times$, et les dérivations successives ou intégrations deviennent de simples facteurs $(j\\omega)$ ou $(1/j\\omega)$. On comprend alors tout l'intérêt d'effectuer : \n",
    "\n",
    "$$ s(t) = h(t) * e(t) \\leadsto S(\\omega) = H(j\\omega) E(\\omega)$$\n",
    "\n",
    "Les années de licence ou de prépa nous ont également habitués à jouer avec des fonctions de transfert. On sait qu'elles ont pour forme habituelle quelque chose du genre : \n",
    "\n",
    "$$ H(j\\omega) = \\frac{b_0 (j\\omega)^n + b_1 (j \\omega)^{n-1} + ... + b_{n-1} (j \\omega) +  b_n }{a_0 (j\\omega)^n + a_1 (j \\omega)^{n-1} + ... + a_{n-1} (j\\omega) + a_n} $$\n",
    "\n",
    "L'exemple général est un peu intimidant, mais si je décide unilatéralement que $b_n = a_n = 1$ et $a_{n-1} = 1/\\omega_0$ et que tous les autres coefficients sont nuls, ma fonction de transfert devient : \n",
    "\n",
    "$$ H(\\omega) = \\frac{1}{1 + j \\omega/\\omega_0} $$ \n",
    "\n",
    "C'est à dire un bête filtre passe-bas d'ordre 1 de fréquence de coupure $\\omega_0$ ! Chouette non ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Afficher les fonctions de transfert : diagrammes de Bode\n",
    "\n",
    "Scipy permet de _visualiser_ la réponse fréquentielle d'un filtre défini avec des $a_i$ et des $b_i$ via la fonction `scipy.signal.freqs()`. Cette fonction demande en entrée :\n",
    "* les listes (ou tableaux) $b$ des coefficients $b_i$ de la puissance la plus élevée [$(j\\omega)^n$] à la plus faible [$(j\\omega)^0$].\n",
    "* la même chose pour la liste $a$ des coefficients $a_i$.\n",
    "* `worN` (lire $\\omega$ or $N$) : le nombre (entier) de fréquences auxquelles vous voulez que votre filtre soit tracé, ou la liste explicite des fréquences qui vous intéressent.\n",
    "\n",
    "La longueur maximale des tableaux $a$ et $b$, notée $n+1$, va définir l'_ordre_ du filtre que vous allez créer, qui sera donc _a priori_ $n$, à part, bien entendu, si vous mettez des zéros partout :-). Examinons le cas de notre filtre d'ordre 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as spsi\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "omega_0 = 3\n",
    "a = [1/omega_0,1]     \n",
    "b = [0,1]\n",
    "\n",
    "#            b_0 (jw) + b_1           1\n",
    "#   H (w) = ----------------- = --------------\n",
    "#            a_0 (jw) + a_1       jw/w_0 + 1\n",
    "\n",
    "w, h_w = spsi.freqs(b,a, worN=np.logspace(-2,2,1000))\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(w, 20*np.log10(np.abs(h_w)))\n",
    "ax.set_title('Bode diagram')\n",
    "ax.set_ylabel('$G_{\\\\rm dB}$')\n",
    "ax.set_xlabel('$\\\\omega$')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vous laisse vérifier qu'on a bien $-20 {\\rm dB}$ par décade aux hautes fréquences, et que le $-3 {\\rm dB}$ est bien atteint pour $\\omega = \\omega_0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Exercices__ : \n",
    "* Créez un filtre passe-haut du deuxième ordre en précisant vous-même les coefficients $a$ et $b$ de votre filtre.\n",
    "* Tracez la réponse _en phase_ du filtre de l'exercice précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ghost of undone homework is getting restless\n",
    "#  .-.\n",
    "# (o o)  \"Doooo youuurrr exerrrciiissseeeesssss\"\n",
    "# | O \\        \n",
    "# \\   \\        \n",
    "#  `~~~'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quelques filtres analogiques\n",
    "\n",
    "Vous vous douterez peut-être que les choses vont _un peu_ se corser si vous voulez appliquer des filtres d'ordre plus élevé, contrôler les facteurs de qualité ou faire de la réjection de bandes, etc. Ne vous inquiétez pas, _Scipy_ est là pour vous aider, et possède en fait des [filtres 'standards'](https://docs.scipy.org/doc/scipy/reference/signal.html#matlab-style-iir-filter-design) qui vont calculer les tableaux $a$ et $b$ pour vous. Par exemple, il est possible d'utiliser des filtres [Butterworth](https://fr.wikipedia.org/wiki/Filtre_de_Butterworth) avec la fonction `scipy.signal.butter()` qui prend en entrée : \n",
    "\n",
    "- L'ordre du filtre $n$\n",
    "- La fréquence de coupure $\\omega_0$ pour les filtres passe-bas ou haut ou les fréquences de coupure $(\\omega_0, \\omega_1)$ pour les filtres passe-bande ou coupe-bande. Celles-ci sont précisées en ${\\rm rad}\\cdot{\\rm s}^{-1}$.\n",
    "- `btype`, qui va préciser le type de filtre, par exemple `'lowpass'`, `'bandpass'`, `'bandstop'`, ...\n",
    "- `analog`, que nous allons laisser à `True` pour l'instant.\n",
    "\n",
    "Tentons de créer un filtre passe-bas d'ordre 2 de cette manière et examinons les coefficients $a$ et $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as spsi\n",
    "\n",
    "b, a = spsi.butter(2, 0.5, analog=True, btype='lowpass')\n",
    "\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de transfert que nous avons obtenue a donc la forme suivante : \n",
    "\n",
    "$$ |H(j\\omega)| = \\frac{1}{-4\\omega^2 + 2 \\sqrt{2} j\\omega  + 1} \\leadsto |H(j\\omega)| = \\frac{1}{\\sqrt{1 + 16 \\omega^4}}$$ \n",
    "\n",
    "C'est à dire la définition d'un filtre [Butterworth](https://fr.wikipedia.org/wiki/Filtre_de_Butterworth) d'ordre 2 avec $\\omega_0 = 0.5~{\\rm rad}\\cdot{\\rm s}^{-1}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Appliquer des filtres analogiques\n",
    "\n",
    "Dessiner des filtres, c'est bien joli, mais comment fait-on pour les _appliquer_ à un signal ? On peut pour cela utiliser _deux_ fonctions : \n",
    "\n",
    "* `scipy.signal.lfilter()` va filtrer le signal une fois\n",
    "* `scipy.signal.filtfilt()` va appliquer _deux fois_ le filtre. Une première fois sur le signal initial à l'endroit, puis ensuite sur le signal filtré une fois mais _retourné temporellement_. \n",
    "\n",
    "Ces fonctions prennent bien en entrée : \n",
    "* les coefficients du numérateur $b$ \n",
    "* ceux du dénominateur, $a$\n",
    "* et le signal $x$ \n",
    "\n",
    "Essayons de filtrer un signal correspondant à une sinusoïde de fréquence 1 Hz à laquelle nous avons ajouté du bruit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as spsi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining variables, then noise and filter\n",
    "t = np.arange(0,6,0.005)                                 # Sampling frequency is 200 Hz\n",
    "noise = 0.25*np.random.uniform(size=len(t))\n",
    "e = np.sin(2*np.pi*t) + noise\n",
    "b, a = spsi.butter(2, 4*np.pi, analog=True)             # Choosing f = 2 Hz for cutoff\n",
    "s = spsi.lfilter(b, a, e)\n",
    "\n",
    "# Plotting area\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t,e, label='Signal Original')\n",
    "ax.plot(t,s, 'r', label='Signal Filtré')\n",
    "ax.legend()\n",
    "plt.ylim([-2,2])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./resources/kevin_horror.jpg)\n",
    "\n",
    "__Que se passe-t-il ?!?__ : En fait, je vous ai un peu menti ... Les coefficients $a$ et $b$ à spécifier lors de l'appel aux fonctions `lfilter()` et `filtfilt()` ne sont pas ceux de l'espace de Fourier ... mais ceux de l'espace de la [transformée en Z](https://fr.wikipedia.org/wiki/Transformation_en_Z) ! En effet, notre signal n'est pas continu, mais _échantillonné_ à temps discret. Je choisis de volontairement ne pas en dire plus.\n",
    "\n",
    "__Pour vous en sortir, vous devrez utiliser la fonction `scipy.signal.bilinear()`__. Celle-ci va prendre en entrée vos $a$ et $b$ analogiques et les transformer en $a_z$ et $b_z$ de la transformée en $Z$. Vous pouvez également lui passer l'argument `fs` correspondant à la fréquence d'échantillonnage de votre signal. Essayons de corriger le tir de la débâcle précédente :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables, then noise and filter\n",
    "t = np.arange(0,6,0.005)                                 # Sampling frequency is 200 Hz\n",
    "noise = 0.25*np.random.uniform(size=len(t))\n",
    "e = np.sin(2*np.pi*t) + noise\n",
    "b, a = spsi.butter(2, 10*np.pi, analog=True)              # Choosing f = 5 Hz for cutoff\n",
    "bz, az = spsi.bilinear(b, a, fs=1/0.005)\n",
    "s = spsi.lfilter(bz, az, e)\n",
    "\n",
    "# Plotting area\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t,e, label='Signal Original')\n",
    "ax.plot(t,s, 'r', label='Signal Filtré')\n",
    "ax.legend()\n",
    "plt.ylim([-2,2])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah ! C'est bien mieux ! Le signal filtré et le signal original se ressemblent déjà bien plus. Cependant, le signal filtré est (légèrement) décalé en temps à cause du _déphasage_ lié au filtre. On peut corriger le tir en appliquant une deuxième fois le filtre, cette fois-ci sur le signal _retourné dans le temps_, puis en remettant _à l'endroit_ le résultat. C'est le concept de la fonction `filtfilt()` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables, then noise and filter\n",
    "t = np.arange(0,6,0.005)                                 # Sampling frequency is 200 Hz\n",
    "noise = 0.25*np.random.uniform(size=len(t))\n",
    "e = np.sin(2*np.pi*t) + noise\n",
    "b, a = spsi.butter(2, 10*np.pi, analog=True)              # Choosing f = 3 Hz for cutoff\n",
    "bz, az = spsi.bilinear(b, a, fs=1/0.005)\n",
    "s = spsi.filtfilt(bz, az, e)\n",
    "\n",
    "# Plotting area\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t,e, label='Signal Original')\n",
    "ax.plot(t,s, 'r', label='Signal Filtré')\n",
    "ax.legend()\n",
    "plt.ylim([-2,2])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Il y avait plus rapide ... les filtres numériques !\n",
    "\n",
    "On peut en fait _directement_ obtenir les coefficients $a$ et $b$ de vos filtres préférés dans l'espace de la transformée en $Z$ si on ... _enlève_ la condition `analog=True` lors de la construction de notre filtre ! Oups ... :-) . Dans un tel cas, \n",
    "\n",
    "* lors de la création de votre filtre, je vous conseille de préciser la fréquence d'échantillonnage $f_s$ de votre signal. Vous pourrez alors choisir des fréquences $f_0$ de coupure entre $0$ et $f_s/2$. Par défaut, $f_s=2$ et $f_c$ est alors comprise entre 0 et 1.\n",
    "* si vous voulez examiner la réponse fréquentielle de votre filtre --cette fois-ci numérique--, vous devrez utiliser la fonction `scipy.signal.freqz()` avec un __z__ et pas un __s__.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables, then noise and filter\n",
    "t = np.arange(0,6,0.005)                                 # Sampling frequency is 200 Hz\n",
    "noise = 0.25*np.random.uniform(size=len(t))\n",
    "e = np.sin(2*np.pi*t) + noise\n",
    "bz, az = spsi.butter(2, 3, fs=1/0.005)                  # Choosing f = 3 Hz for cutoff\n",
    "s = spsi.filtfilt(bz,az,e)\n",
    "\n",
    "# Plotting area\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t,e, label='Signal Original')\n",
    "ax.plot(t,s, 'r', label='Signal Filtré')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercice__ : Le signal suivant contient trois composantes en fréquence ($0.2$, $1$ et $3$ Hz). Essayez de définir un filtre numérique permettant de ne garder que la composante à 1 Hz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0,5,0.005)\n",
    "e = np.sin(2*np.pi*t) - 2*np.sin(2*np.pi*0.2*t) + 0.3*np.cos(2*np.pi*3*t) \n",
    "\n",
    "# Plotting area : add your filtered plot here !\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t,e)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtrage avec fenêtres\n",
    "\n",
    "Si vous êtes plutôt du genre _espace direct_, vous pouvez également faire appel à des fonctions de fenêtrage explicites $h(t)$ parmi la [longue liste](https://docs.scipy.org/doc/scipy/reference/signal.windows.html) de fenêtres disponibles. Un choix assez classique consiste à choisir la fenêtre de [Hann](https://en.wikipedia.org/wiki/Hann_function), `scipy.signal.windows.hann()`, qui ne prend en entrée qu'un nombre $M$ de points, c'est à dire le nombre de points sur lequel on va effectuer le filtrage. Comme vous l'aurez constaté, ces fenêtres vont principalement _lisser_ le signal en effectuant un filtrage _passe-bas_ typiquement sur $M$ points, c'est à dire avec une fréquence de coupure de l'ordre de $f_s/M$.\n",
    "\n",
    "Pour filtrer le signal avec ces fenêtres, on va ensuite simplement appeler la fonction `np.convolve()` ou `scipy.signal.convolve()` afin de calculer : \n",
    "\n",
    "$$ s(t) = (h * e) (t)$$\n",
    "\n",
    "Par défaut, ces fenêtres ne sont pas normalisées, donc faites un tour par la case `sum()` pour la normaliser et voir votre filtre en action. Le tour est joué !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as spsi\n",
    "\n",
    "t = np.arange(0,6,1/100)            # Sampling frequency : 100 Hz\n",
    "noise = 0.25*np.random.normal(size=len(t))\n",
    "e = np.sin(2*np.pi*t) + 0.25*noise\n",
    "window = spsi.windows.hann(20)      # Cutoff frequency : 100/20 = 5 Hz\n",
    "window_norm = window/sum(window)\n",
    "s = np.convolve(e, window_norm, mode='same')\n",
    "\n",
    "# Plotting area\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t,e, label='Signal Original')\n",
    "ax.plot(t,s, 'r', label='Signal Filtré')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercice__ (difficile) : Essayez de développer une fenêtre permettant de dériver un signal (aux basses fréquences) tout en filtrant les hautes fréquences. Vous pouvez partir de la relation bien connue des [amateurs de distributions](https://fr.wikipedia.org/wiki/Distribution_(math%C3%A9matiques)) : \n",
    "\n",
    "$$ (e * h')(t) = \\int e(\\tau) h'(t - \\tau) {\\rm d}\\tau = - \\int e'(\\tau) h(t - \\tau)~{\\rm d}\\tau = (e' * h) (t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *Eye of the Tiger starts playing in the background*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Le filtre de Savitsky-Golay\n",
    "\n",
    "Dans la catégorie 'inclassables', le filtre de [Savistky-Golay](https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter) est un filtre assez pratique qui va essayer de fitter par un polynôme d'ordre assez faible des bouts de taille $M$ de votre signal à filtrer. Ce filtrage par des polynômes est assez intéressant car il permet en outre de calculer la dérivée de votre signal initial tout en limitant le bruit habituellement associé au calcul des dérivées.\n",
    "\n",
    "La fonction `spsi.savgol_filter()` prend donc naturellement comme arguments : \n",
    "* $e$, le signal à filtrer\n",
    "* $M$, le nombre de points des bouts de signaux à fitter : c'est en fait la 'taille' du filtre. Ce nombre doit être impair.\n",
    "* `polyorder`, l'ordre du polynôme utilisé pour ajuster les données\n",
    "* `deriv` [optionnel] : précisez un entier $n$ pour obtenir la $n^{\\rm ème}$ dérivée du signal filtré.\n",
    "\n",
    "Le filtre renvoie alors directement les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as spsi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Signal construction\n",
    "t = np.linspace(-10,10,200)\n",
    "noise = 0.15*np.random.normal(size=len(t))\n",
    "e = noise + 2*t*np.exp(-t**2) + 0.1*t\n",
    "\n",
    "# Signal filtering\n",
    "s = spsi.savgol_filter(e, window_length=31, polyorder=3)\n",
    "\n",
    "# Plotting area\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, e, label='Original')\n",
    "ax.plot(t, s, 'r', label='Savitsky-Golay')\n",
    "ax.legend()\n",
    "ax.set_ylim([-1.5,1.5])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "\n",
    "### Interpolation\n",
    "\n",
    "L'interpolation de données s'effectue généralement lorsque les signaux qu'on mesure ne sont pas échantillonnés régulièrement. Certaines techniques, par exemple la [vélocimétrie laser Doppler](https://fr.wikipedia.org/wiki/V%C3%A9locim%C3%A9trie_laser), mesurent la vitesse locale dans un fluide ... mais seulement lorsqu'une particule traverse le faisceau laser ! Vous vous douterez bien que les traceurs ne vont pas gentiment s'organiser pour passer devant le détecteur à intervalles réguliers, et il est alors difficile, par exemple, de calculer le spectre de Fourier d'un tel signal.\n",
    "\n",
    "Dans certains autres cas, il est parfois utile d'interpoler le signal pour avoir _plus de points_ de données que dans le jeu de données initial. Cela permet de combler certains trous et d'avoir un signal ou une image finaux d'aspect plus lisse, plus agréable à l'oeil.\n",
    "\n",
    "Nous allons voir ici ce que le module `scipy.interpolate()` a dans le ventre !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation à une dimension \n",
    "\n",
    "L'interpolation à une dimension est une affaire assez simple et s'effectue avec la fonction `scipy.interpolate.interp1d()`. Vous aurez besoin pour cela de lui préciser : \n",
    "* les valeurs de $x$ sur lesquelles la fonction est calculée, qui n'est pas forcément trié.\n",
    "* les valeurs de $y = f(x)$ de votre fonction aux $x$ précédemment mentionnés\n",
    "* le _type_ (`kind`) d'interpolation, qui peut être au plus proche voisin (`'nearest'`), linéaire (`'linear'`), avec des splines cubiques (`'cubic'`), ...\n",
    "* le comportement à avoir par rapport à l'extrapolation, avec `fill_value`: vous pouvez choisir un ou plusieurs nombres, ou préciser que vous voulez extrapoler les données avec `'extrapolate'`.\n",
    "\n",
    "_La fonction `interp1d()` renvoie en retour un interpolateur_ : c'est l'estimation de la fonction $f$ obtenue par l'ordinateur. En pratique, c'est un objet à qui on va ensuite donner à manger la liste des points $x_i$ où l'on veut interpoler les données, et qui nous recrache les $f(x_i) = y_i$ qui nous intéressent. \n",
    "\n",
    "Essayons d'interpoler une fonction arbitraire à partir de données mal échantillonnées en $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.interpolate as spint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.random.uniform(size=100) # Irregular x values\n",
    "y = 5*x*np.exp(-5*x)\n",
    "\n",
    "interpolator = spint.interp1d(x, y, kind='linear', fill_value='extrapolate')\n",
    "xi = np.linspace(0,1,100)\n",
    "yi = interpolator(xi)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xi, yi, 'r')\n",
    "ax.plot(x, y, 'ko', markerfacecolor='k', markersize=2)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces interpolateurs peuvent être très puissants pour atteindre la [_précision sous-pixel_](https://dsp.stackexchange.com/questions/34103/subpixel-what-is-it) qui est parfois recherchée pour localiser des petits déplacements de particules ou des légers déphasages entre deux signaux. \n",
    "\n",
    "Prenons un signal $y_i$ qui possède un pic théorique étroit en $x_0=0.02$, mais qui n'est pas échantillonné en $x = x_0$. En l'interpolant par une [_spline_](https://fr.wikipedia.org/wiki/Spline), on va essayer de _retrouver_ la position du maximum théorique .\n",
    "\n",
    "_Note_ : Cette méthode _ne fonctionne pas_ avec des interpolations linéaires ou de plus proche voisin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-0.4,0.4,0.05)\n",
    "noise = 0.01*np.random.uniform(size=len(x)) # Add a bit of noise to show the method is not perfect\n",
    "y = np.sinc(3*(x-0.02))**2 + noise\n",
    "x_0_naive = x[np.argmax(y)]\n",
    "\n",
    "# Create the interpolator, then evaluate it close to x_0 with a smaller δx\n",
    "interpolator = spint.interp1d(x, y, kind='cubic')\n",
    "xi = np.linspace(x_0_naive-0.05,x_0_naive + 0.05,101)\n",
    "yi = interpolator(xi)\n",
    "\n",
    "# Check where we have found our maxima / compare with naive method\n",
    "x_0_interp = xi[np.argmax(yi)]\n",
    "print(f'Naive        estimate of maximum : {x_0_naive:.3f}')\n",
    "print(f'Interpolated estimate of maximum : {x_0_interp:.3f}')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x , y , 'ko', markerfacecolor='k', markersize=2)\n",
    "ax.plot(xi, yi, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous répétez l'exécution de l'algorithme plusieurs fois, vous verrez que le maximum interpolé est bien détecté aux alentours de $x_0 = 0.02$ avec une variabilité de l'ordre de $\\pm 0.003$, alors que le signal initial était faux de $-0.02$ ! \n",
    "\n",
    "Pour ceux que cela embêterait, sachez que je n'ai pas extrait de l'information du néant pour obtenir ma valeur du maximum. Les _splines_ qui interpolent le signal entre $x_i$ et $x_{i+1}$ ne se basent pas jsute sur les valeurs $y_i$ et $y_{i+1}$. Elles exploitent également les informations du 'voisinage', c'est à dire $y_{i-1}, y_{i-2}, ..., y_{i+1}, y_{i+2}, ...$, qu'elles utilisent pour reconstruire plus fidèlement la fonction $f$.\n",
    "\n",
    "Ce procédé de détection _sub-pixel_ de maxima est tout à fait applicable aux images à deux ou trois dimensions. Pratique pour déterminer le déplacement de petites particules sur de petites distances, ... par exemple dans un système de pinces optiques !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercice__ (difficile !) : Essayez de déterminer le décalage (en millisecondes) entre les deux signaux d'électrocardiogrammes $y_1$ et $y_2$ du code ci-dessous. Calculez pour cela la corrélation croisée entre les signaux. _Note_ : Veillez à _normaliser_ vos variables avant d'essayer de les corréler :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import scipy.interpolate as spint\n",
    "\n",
    "# Loading the file\n",
    "with open('./files/ecg_signal.json') as myfile:\n",
    "    data = json.load(myfile)\n",
    "t, y1, y2 = data['t'], data['y1'], data['y2']\n",
    "\n",
    "# Wait ... so much code and it is not even resolved yet ? Jeeeeeezzzzz ...."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inerpolation à deux dimensions\n",
    "\n",
    "L'interpolation à 2d ~~s'effectue assez facilement également avec la fonction `scipy.interpolate.interp2d()`~~ ... Perdu ! En fait, cette fois-ci, tout dépend de vos données initiales. Si celles-ci sont déjà _déjà placées sur une grille_ et que vous voulez les interpoler entre les points initiaux, vous pourrez utiliser `scipy.interpolate.RectBivariateSpline()`. Sinon, si vos données initiales sont éparpillées à des positions $(x,y)$ pas régulières, vous devrez utiliser `scipy.interpolate.griddata()`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Données structurées : `RectBivariateSpline()`\n",
    "\n",
    "Essayons de dé-pixelliser une image d'un joli raton-laveur (ou tout du moins de son oeil). La fonction `scipy.interpolate.RectBivariateSpline()` est très bien adaptée aux images qui sont naturellement échantillonnées sur une grille bien régulière. La fonction est en plus assez similaire à `interp1d()` et va fabriquer à nouveau un interpolateur, qui aura cette fois-ci besoin d'une échelle en $x$ et en $y$. Vous ne serez pas dépaysé.e.s ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.interpolate as spint\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import face     # Source of the image\n",
    "\n",
    "# Generation and cropping of initial figure around a point of interest\n",
    "x = np.arange(690,750)\n",
    "y = np.arange(300,360)   \n",
    "image = face(gray=True)[y[0]:y[-1]+1,x[0]:x[-1]+1] \n",
    "\n",
    "# Interpolation \n",
    "interpolator = spint.RectBivariateSpline(x,y,image)\n",
    "x_refined = np.linspace(x[0],x[-1],500) # Refined --> more points than initially (60)\n",
    "y_refined = np.linspace(y[0],y[-1],500) # Refined --> more points than initially (60)\n",
    "image_refined = interpolator(x_refined, y_refined)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "ax[0].imshow(image, cmap='gray')\n",
    "ax[1].imshow(image_refined, cmap='gray')\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].set_title('Interpolated ')\n",
    "fig.set_size_inches(w=13, h=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre image initiale est bien pixellisée, on voit d'ailleurs les gros pixels en question. L'image interpolée, elle, possède bien plus de pixels (cf. sur les axes), même si elle reste _floue_, car on ne peut à nouveau pas retrouver de l'information qui n'existait pas initialement :-) . L'interpolation a donc bien fonctionné."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Données échantillonnées aléatoirement : `griddata()`\n",
    "\n",
    "Cette fois-ci, vous devrez simplement préciser les différents points $(x,y)$ où votre $f$ existe, et la fonction va se charger du reste ! Choisissons des coordonnées arbitrairement, puis calculons un $f(x,y)$ à ces endroits, avant d'effectuer l'interpolation.\n",
    "\n",
    "La fonction `scipy.interpolate.griddata()` demande :\n",
    "- des couples de coordonnées $(x,y)$ au format $N \\times 2$\n",
    "- la valeur de $z$ à ces coordonnées au format $N \\times 1$\n",
    "- les couples de nouvelles coordonnées $(x_i, y_i)$ auxquelles on souhaite calculer la fonction, au format $P \\times 2$\n",
    "- une méthode d'interpolation : `linear`, `cubic`, ...\n",
    "\n",
    "Vous aurez donc à la fin également une liste de $P \\times 1$ valeurs, que vous ne pourrez pas directement tracer comme surface. Pour quand même tracer une jolie surface, on va :\n",
    "1. Créer nos grilles de coordonnées à deux dimensions $X_i$ et $Y_i$. \n",
    "2. On va ensuite aplatir ces grilles de coordonnées pour les transformer en listes 1d de coordonnées $x_i$ et $y_i$\n",
    "3. Joindre ces données ensemble pour fabriquer $(x_i, y_i)$, que l'on va passer à `griddata()`\n",
    "4. Re-transformer les données finales $f(x_i, y_i)$ en tableau 2d de même taille que la grille des $X_i$ et $Y_i$\n",
    "5. Tracer les résultats, et s'apprécier pour le travail accompli \n",
    "\n",
    "![img](./resources/Self-love.jpg)\n",
    "\n",
    "(image fournie par [Freepik](https://www.freepik.com/free-vector/high-self-esteem-illustration-with-woman-leaves_10781893.htm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.interpolate as spint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xy = 6*np.random.uniform(size=(300,2))-3            # Arbitrary (x,y) coordinates between -3 and 3\n",
    "data = np.exp(-1/( (xy[:,0])**2 + (xy[:,1])**2))    # Well-shaped function :-)\n",
    "\n",
    "interp_range = np.linspace(-3,3,100)\n",
    "X_i, Y_i = np.meshgrid(interp_range, interp_range)      # Create 2d grid of X, Y coordinates\n",
    "xy_i = np.transpose([np.ravel(X_i), np.ravel(Y_i)])     # Create P x 2 list of (x,y) coordinates\n",
    "data_i = spint.griddata(xy, data, xy_i)                 # Interpolate the data\n",
    "data_i_grid = np.reshape(data_i, X_i.shape)             # Put the interpolated data back on a grid\n",
    "\n",
    "# 3d figure plotting\n",
    "fig = plt.figure(figsize=[10,8])\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot(xy[:,0], xy[:,1], data, 'k.', markersize=2)\n",
    "ax.plot_surface(X_i, Y_i, data_i_grid, cmap='magma', alpha=0.5)\n",
    "ax.set_title('Interpolation from initial points (black) using griddata()')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolations à $d > 2$ \n",
    "\n",
    "Bonne nouvelle ! Vous pouvez utiliser `griddata()` pour interpoler des données à plus que deux dimensions. En fait, la fonction s'applique en dimensions arbitraires. Il faut juste remplacer votre couple de coordonnées $(x,y)$ de taille $N \\times 2$ par une liste $(x,y,z,t,w,\\ldots)$ de taille $N \\times d$, et faire de même pour les coordonnées interpolées $(x_i,y_i,z_i,t_i,w_i,\\ldots)$, mais sinon la syntaxe est exactement la même. \n",
    "\n",
    "Pour les données disposées initialement sur une grille, vous pouvez essayer d'utilier la fonction `scipy.interpolate.interpn()`, qui est un peu un amalgame entre `griddata()` et `RectBivariateSpline()` : \n",
    "\n",
    "- en premier, on lui précise un tuple qui contient les $x$ (taille $N_x \\times 1$), $y$ (taille $N_y \\times 1$), $z$ (taille $N_z \\times 1$), ... \n",
    "- ensuite, les valeurs de la fonction en $(x,y,z,\\ldots)$, donc un tableau de taille $N_x \\times N_y \\times N_z \\times \\ldots$\n",
    "- enfin, la liste des points auxquels vous voulez calculer les données, qui est donc au format $P \\times d$ si vous travaillez en dimension $d$. \n",
    "\n",
    "Je vous laisse un court exemple à ce sujet pour que vous évitiez de trop galérer avec la syntaxe. Si votre code déconne, vérifiez bien que la longueur des $(x,y,z,...)$ sont bien dans le même ordre que les dimensions de votre tableau de données initial (`values` dans l'exemple suivant). Si ça ne va pas, un petit coup de `indexing='ij'` dans `np.meshgrid()` ou une permutation de `(x,y,z)` en `(y,x,z)` dans l'appel à `interpn()` devrait faire l'affaire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interpn\n",
    "\n",
    "x = np.linspace(0, 4, 5)\n",
    "y = np.linspace(0, 5, 6)\n",
    "z = np.linspace(0, 6, 7)\n",
    "\n",
    "X,Y,Z = np.meshgrid(x, y, z, indexing='ij')\n",
    "values = X**2 + Y - 3*Z\n",
    "eval_points = np.array([[3,3,2], [2,3,4], [4,5,6], [0,1,2]])\n",
    "interpolated_vals = interpn((x,y,z), values, eval_points)\n",
    "\n",
    "print('Initial data shape : ' + str(np.shape(values)))\n",
    "print('Initial x,y,z, ... shapes : ' + str(len(x)) + ', ' + str(len(y)) + ', ' + str(len(z)))\n",
    "print('Interpolated values : ' + str(interpolated_vals))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------\n",
    "\n",
    "### Maxima, Minima et Zeros\n",
    "\n",
    "`Scipy` vous permet de trouver les maxima, les minima ou les zéros de séries de données à une ou plusieurs variables, c'est à dire de tableaux de données à $1,2,\\ldots$ dimensions, mais également de fonctions -- dont on n'a _a priori_ pas encore calculé les valeurs. Cette section va donc être divisée en deux parties en fonction du type d'entrée dont vous souhaitez extraire les minimax ou zéros. Vous conviendrez d'emblée que rechercher des maxima d'une fonction $f(x,y,\\ldots)$ revient à calculer les minima de $-f(x,y,\\ldots)$, donc la recherche de minima et de maxima revient en fait _grosso modo_ au même. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimax de séries de données \n",
    "\n",
    "Il existe deux fonctions pour détecter les pics locaux de séries de données, en fonction du nombre de dimensions du signal initial : \n",
    "\n",
    "* Pour une série 1d, on va utiliser la fonction `scipy.signal.find_peaks()`\n",
    "* Pour des tableaux 2d et plus, on peut utiliser la fonction `scipy.ndimage.extrema()` pour récupérer le minimum et le maximum _global_ d'un tableau. Si vous voulez récupérer des pics locaux d'une série 2d, il existe également une fonction `peak_local_max()` dans le module [Scikit-Image](./Application_E_ScikitImage.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maxima d'un signal 1d avec `find_peaks()`\n",
    "\n",
    "La fonction `scipy.signal.find_peaks()` va vous lister tous les pics d'un signal $s(t)$. Par défaut, c'est simplement tous les points pour lesquels la valeur de $s(t)$ est plus élevée que celle de ses deux voisins, triés de gauche à droite (et non du pic le plus grand au plus petit). Par exemple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as spsi\n",
    "\n",
    "s = np.random.normal(size=100)\n",
    "peaks,_ = spsi.find_peaks(s) # Don't forget the \", _\" !\n",
    "\n",
    "# Figure\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(s)\n",
    "ax.plot(peaks, s[peaks], 'r^')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela fait _beaucoup_ de pics ... et vous voudrez peut-être ne conserver que les plus hauts, ou les plus élevés d'entre eux. Pour cela, vous pouvez vous appuyer sur certaines options : \n",
    "\n",
    "* `height`, qui va préciser l'ordonnée minimale à laquelle le pic doit se trouver\n",
    "* `distance`, qui indique la distance minimale (en nombre de points) entre pics\n",
    "* `prominence`, la [_proéminence_](https://fr.wikipedia.org/wiki/Pro%C3%A9minence) d'un pic, qui indique en gros de combien il faut que l'on 'redescende' d'un pic pour aller trouver un autre pic plus haut que celui-ci ou atteindre les bords du signal.  \n",
    "* `threshold`, qui va indiquer de combien le pic doit être plus haut que les valeurs avoisinantes pour être comptabilisé. \n",
    "\n",
    "Pour `height`, `prominence` et `threshold`, vous pouvez préciser à la fois une limite _inférieure et supérieure_ en donnant à manger aux options une liste à deux éléments. \n",
    "\n",
    "__Exercice__ : Essayez donc de ne conserver que les six pics les plus hauts du signal suivant, puis les six pics les plus proéminents. Obtient-on les mêmes résultats ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as spsi\n",
    "\n",
    "np.random.seed(1987)\n",
    "s = np.random.normal(size=100)\n",
    "peaks,_ = spsi.find_peaks(s) # Don't forget the \", _\" !\n",
    "\n",
    "# Figure\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(s)\n",
    "ax.plot(peaks, s[peaks], 'r^')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maximum d'un signal $d\\geq 2$ avec `extrema()`\n",
    "\n",
    "Cette fois-ci, la fonction est un peu plus rustique, et vous ne pourrez que lister le minimum et le maximum de votre tableau à $n$ dimensions. Si vous voulez obtenir les minimax _locaux_ à deux dimensions, jetez plutôt un oeil à la fonction `peak_local_max()` du module [Scikit-Image](./Application_E_ScikitImage.ipynb). La fonction vous renvoie un tuple `(min, max, loc_min, loc_max)`. Par exemple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as spnd\n",
    "\n",
    "z = np.zeros((3,3,3))\n",
    "z[2,2,0] = -2\n",
    "z[0,2,1] = 3\n",
    "\n",
    "min, max, loc_min, loc_max = spnd.extrema(z)\n",
    "print(f'Minimum : {min} found at index {loc_min}')\n",
    "print(f'Maximum : {max} found at index {loc_max}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimisation et zéros de fonctions\n",
    "\n",
    "Que se passe-t-il si, maintenant, au lieu de chercher un maximum dans votre série de données, vous auriez besoin de minimiser une _fonction_, c'est à dire trouver le minimum _exact_ (autant que possible avec Python) d'une fonction ? Ou de trouver les endroits où elle s'annule ? Les fonctions Scipy qui recherchent les extrema et les zéros s'utilisent avec une syntaxe similaire, mais sachez que pour le calcul des minimax, il vous faudra des fonctions _scalaire_, c'est à dire du type : \n",
    "\n",
    "$$f: \\mathbb{R}^n \\to \\mathbb{R}$$\n",
    "\n",
    "La fonction $f$ peut posséder de nombreuses variables, mais son résultat doit être un nombre. Pour le calcul des zéros, c'est un peu différent, car on a, même avec Scipy, besoin de $n$ équations pour déterminer $n$ variables :-). Les fonctions qui sont admissibles pour la recherche de zéros sont donc du type : \n",
    "\n",
    "$$h: \\mathbb{R}^n \\to \\mathbb{R}^n$$\n",
    "\n",
    "Les fonctions `scipy.optimize.minimize()` et `scipy.optimize.root()` (ainsi que leurs variants _scalaires_, `..._scalar()`) possèdent quasiment tous une syntaxe similaire en entrée, du type : \n",
    "\n",
    "* `fun` : la fonction à minimiser\n",
    "* `x0` [parfois optionnel] : le $(x,y,\\ldots)$ de départ pour l'algorithme\n",
    "* `bounds` [optionnel] : les limites en $(x,y,\\ldots)$ pour trouver le minimum \n",
    "\n",
    "Comment définir ma fonction $f$ en Python si on ne calcule pas ses valeurs ? Eh bien ... on va utiliser les fonctions Python. Eh, oui, on ne va pas réinventer la roue :-). On va donc créer nous-même $f$ avec une `def`, et on va _directement_ passer la fonction Python $f$ à la fonction `minimize_scalar()` qui va tenter de la minimiser.\n",
    "\n",
    "Ces fonctions vont vous renvoyer un objet `result` (de type `OptimizeResult`, ce qui nous fait une bonne jambe), et qui contient quasiment toujours : \n",
    "* Un message, `result.message` (de type `str`)\n",
    "* Un indicateur de la réussite de la recherche, `result.success` (un `bool`)\n",
    "* La position (éventuelle) du minimum trouvé, $x$ avec `result.x`\n",
    "* La valeur de $f$ au minimum $x$, `result.fun`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minima à une dimension : `minimize_scalar()`\n",
    "\n",
    "Choisissons une fonction biscornue $f(x) = \\Gamma(x)/x^3$ avec [$\\Gamma$ la fonction spéciale bien connue](https://fr.wikipedia.org/wiki/Fonction_gamma). Je sais que $\\Gamma(x) \\to \\infty$ pour $x\\to 0$ et $x \\to \\infty$ et que $f$ en fait de même, la fonction doit donc posséder un minimum. Je peux demander à `scipy.optimize.minimize_scalar()` de calculer le minimum de cette fonction pour moi. Cette fonction, contrairement à d'autres, n'a pas besoin d'un point de départ `x0`. Trouver son minimum va donc revenir à écrire :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special as spec\n",
    "import scipy.optimize as spop\n",
    "\n",
    "def f(x):\n",
    "    return spec.gamma(x)/x**3\n",
    "\n",
    "result = spop.minimize_scalar(f, bounds=[0.1,10])\n",
    "print('Minimum found at x = {:.3g}, f(x) = {:.3g}'.format(result.x, result.fun))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercice__ : Essayez de déterminer le _maximum et le minimum_ de :\n",
    "\n",
    "$$g(x) = \\frac{x \\ln(x)}{\\exp(x)}$$\n",
    "\n",
    "Puis tracez la fonction $g$ et affichez la position en $x$ et $y$ du maximum et du minimum de $g$ en surimpression sur le graphe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piece of cake, mate. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minima a plusieurs dimensions : `minimize()`\n",
    "\n",
    "La syntaxe est ici très similaire à ce que nous venons de voir, même si les objets sont plus compliqués à visualiser. Essayons avec la fonction :\n",
    "\n",
    "$$ g(x,y) = (1-x)^2 + 100(x - y^2)^2 $$\n",
    "\n",
    "Cette fonction est appelée fonction de [Rosenbrock](https://fr.wikipedia.org/wiki/Fonction_de_Rosenbrock), et possède un minimum très peu marqué en $(1,1)$.\n",
    "\n",
    "La fonction `scipy.optimize.minimize()` fonctionne comme `minimize_scalar()`, hormis le fait que l'argument `x0` [qui est en fait $(x_0, y_0, \\ldots)$] est nécessaire en entrée, et que l'objet renvoyé par `minimize()` contient plus d'informations, notamment sur les dérivées de $g$ au minimum trouvé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as spop\n",
    "\n",
    "g = spop.rosen      \n",
    "    # Our g is simply the Rosenbrock function with no () because we\n",
    "    # are not computing any values yet \n",
    "result = spop.minimize(g, x0=[0,0], bounds=[[-10,10],[-10,10]])\n",
    "print(f'Minimum found at x, y = {result.x}, g(x,y) = {result.fun:.3f}')\n",
    "\n",
    "# Graphical representation\n",
    "fig, ax = plt.subplots()\n",
    "extent = np.linspace(-3,3,300)\n",
    "X, Y = np.meshgrid(extent, extent)\n",
    "# Do not forget to set origin='lower' otherwise image will be flipped !\n",
    "# Also specify the 'extent' option matching your $x$, $y$ range\n",
    "ax.imshow(g((X,Y)), extent=[-3,3,-3,3], origin='lower', vmax=100) \n",
    "ax.plot(result.x[0], result.x[1], 'rs')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retrouve donc bien notre maximum en $(1,1)$.\n",
    "\n",
    "_Note_ : si vous n'êtes pas content.e.s de la position du minimum que vous avez trouvé, vous pouvez essayer de partir d'un autre point de départ $(x_0, y_0,\\ldots)$. Si cela ne fonctionne pas _non plus_, vous pouvez essayer des algorithmes qui fonctionnent différemment, comme :\n",
    "* [`scipy.optimize.basinhopping()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.basinhopping.html#scipy.optimize.basinhopping), qui est capable de 'sauter' entre différentes 'vallées' pour ne pas rester bloqué dans un minimum local qui n'est pas global\n",
    "* [`scipy.optimize.dual_annealing()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.dual_annealing.html#scipy.optimize.dual_annealing), qui va simuler une particule 'brownienne' qui va tomber dans le puits de potentiel correspondant à votre fonction lors d'un recuit simulé. \n",
    "* [`scipy.optimize.brute()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brute.html#scipy.optimize.brute), qui va chercher par force brute le minimum de votre fonction en la calculant à intervalles réguliers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zéros d'une fonction à une dimension `root_scalar()`\n",
    "\n",
    "Cherchons le ou les zéros de la fonction :\n",
    "\n",
    "$$ h(x) = 1 + x - 2\\cos x $$\n",
    "\n",
    "Si on réfléchit au signe de la dérivée de $h$, on a plusieurs zéros, et la fonction `scipy.optimize.root_scalar()` va trouver _un seul_ d'entre eux. Intuitivement, on se dit que celui qui sera trouvé est le plus proche du point de départ, non ? Essayez de changer le `x0` dans le code suivant, exemple en choisissant $x_0 = 0$, puis $-0.75$, $-0.80$, $-0.95$, $-1$ et examinez le résultat : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as spop\n",
    "\n",
    "def h(x): return 1 + x - 2*np.cos(x)\n",
    "\n",
    "# Root finding\n",
    "x0 = -0\n",
    "result = spop.root(h, x0=x0)\n",
    "\n",
    "# Figure\n",
    "x_list = np.linspace(-5,5,500)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_list, h(x_list))\n",
    "ax.plot(x_list, 0*x_list, 'k:')\n",
    "ax.plot(x0, h(x0), 'g>', label='Initial point')\n",
    "ax.plot(result.x, result.fun, 'rs', label='Zero of $h$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le maximum trouvé n'est pas forcément celui le plus proche du point de départ, donc faites attention et veillez à tester un peu en détail la _robustesse_ de votre résultat ! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zéros d'une fonction plusieurs dimensions : `root()` \n",
    "\n",
    "Considérons une fonction $h$ à plusieurs variables, $(x,y)$ et qui renvoie elle-même deux valeurs : \n",
    "\n",
    "$$ h (x,y) = \\left (\\begin{array}{c} \\exp(x) - y \\\\ y^2 - 5 \\end{array}\\right ) $$\n",
    "\n",
    "Si on se creuse un peu la tête, on peut voir que la solution de $h(x,y) = (0,0)$ revient en fait à écrire $y = \\exp(x) = \\sqrt{5}$. Comment s'en sort notre algorithme ? Ma foi, plutôt bien : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as spop\n",
    "\n",
    "def h(xy):\n",
    "    x,y = xy[0], xy[1]\n",
    "    return [np.exp(x) - y, y**2 - 5]\n",
    "\n",
    "result = spop.root(h, x0=[0.5,0.5])\n",
    "\n",
    "print(f'Zero found at x, y = {result.x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois de plus, si vous savez que votre fonction _a de nombreux zéros_, faites attention à votre point de départ !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "\n",
    "### Ajustements et _fits_\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ajustement et le _fit_ de données est, en fait, une simple optimisation (minimisation) de fonction, mais comme je vous connais un peu, je me doute que vous n'allez pas vraiment regarder dans cette section pour aller fitter des données :-). Donc je fais une petite section à ce sujet ici. \n",
    "\n",
    "Le concept du fit n'est pas super difficile à comprendre : vous avez des données $x_i$, $y_i$ et vous tentez de faire passer une courbe du type $f(x)$ à travers les $x_i$. Dans le cas le plus idéal, vous avez exactement $f(x_i) = y_i$, mais en pratique vous aurez toujours du bruit dans les données, des petites erreurs systématiques. Dans le cas d'un bon fit, $f(x_i) - y_i$ ne va jamais être nul, mais petit. L'objectif des fonctions de fit est de trouver une fonction $f$ qui minimise la distance $D$ entre $f(x_i)$ et les $y_i$ : \n",
    "\n",
    "$$ D = \\sum_i \\left | f(x_i) - y_i \\right |^2 $$\n",
    "\n",
    "Bien entendu, on ne peut pas laisser Scipy choisir n'importe quelle fonction $f$ : on doit avoir une idée de la fonction à ajuster (_fitter_). On va en fait ajouter des _paramètres_ $a,b,c,\\ldots$ dans nos fonctions $f_{a,b,c,\\ldots}(x)$. Scipy peut alors tenter de minimiser $D(a,b,c,\\ldots)$.\n",
    "\n",
    "Dans cette section, on va d'abord regarder comment définir notre distance $D$ à la main pour ensuite _fitter_ une courbe à des données avec `scipy.optimize.least_squares()`, puis ensuite nous verrons qu'on peut plus directement _fitter_ $f$ à des données sans définir $D$ via `scipy.optimize.curve_fit()`. Si vous voulez faire des simples ajustements linéaires, vous pouvez jeter un oeil à [`scipy.optimize.lsq_linear()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html#scipy.optimize.lsq_linear), voire à la méthode [`np.linalg.lstsq()`](./Application_A_Numpy.ipynb/#Regression-lineaire) vue dans l'Application A.\n",
    "\n",
    "En pratique : sachez qu'au delà de deux paramètres, les _fits_ vont quasiment toujours fonctionner, et que leur convergence va être de plus en plus délicate. Les algorithmes de _fit_ ont également des difficultés lorsque deux paramètres ont quasiment le même effet sur la fonction $f$. \n",
    "\n",
    "_Note_ : Dans la vie réelle, vous n'aurez pas toujours d'expression explicite $y=f(x)$, mais plutôt $x = g(y)$ ... et rien ne vous empêche de fitter $g$ ! Si, par contre, vous avez une expression du type $h(x,y, \\ldots)=0$, alors, euh, là \n",
    "\n",
    "## implicit fit with h(x,y)=0 ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimisation d'une distance $D$ explicite : `minimize()` et `least_squares()`\n",
    "\n",
    "Partons d'une série de données $y_i$. Comme je construis ici une série de données de toutes pièces, je vais écrire : \n",
    "\n",
    "$$ y_i = f_* (x_i) + \\zeta_i$$\n",
    "\n",
    "C'est à dire que $y_i$ est la somme d'une _fonction à retrouver_ $f_*$ et d'un bruit $\\zeta$. A priori, je ne suis pas censé connaître $f_*$, mais j'ai une vague idée de ce à quoi elle ressemble, quelque chose du type :\n",
    "\n",
    "$$ f_{a,b}(x) = \\exp(-a x) \\sin(b x) $$ \n",
    "\n",
    "On veut retrouver les valeurs de $a$ et $b$. Dans notre code Python, on va donc définir une 'fonction' (au sens Python) définissant les fonctions (mathématiques) $f_{a,b}$. On va donc devoir donner à manger à la fonction Python `f` en entrée : \n",
    "* les valeurs `x` auxquelles la fonction sera évaluée plus tard\n",
    "* les paramètres `prms` $a$ et $b$, que je mets dans une liste. Ils vont venir modifier la forme des $f$\n",
    "\n",
    "Je peux utiliser les $f$ avec un jeu de `prms` bien précis pour créer les $y_i$. Je peux ensuite définir une nouvelle fonction Python `D` qui prend en entrée `prms` et renvoie la somme des carrés des `y_i - f(x_i,prms)`. On peut alors simplement minimiser $D$ en utilisant [`scipy.optimize.minimize()` vue précédemment](#minima-a-plusieurs-dimensions--minimize). Celle-ci demande en entrée la fonction à minimiser, `D`, et un point de départ pour vos paramètres, `x0` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as spop\n",
    "\n",
    "def f(x, prms):\n",
    "    return np.exp(prms[0]*x)*np.sin(prms[1]*x)\n",
    "\n",
    "def D(prms):\n",
    "    return np.sum((y_i - f(x_i, prms))**2)\n",
    "\n",
    "# Data generation\n",
    "x_i = np.linspace(0,6,30)\n",
    "y_i = f(x, [-0.5,np.pi]) + 0.2*np.random.normal(scale=0.2,size=len(x_i))\n",
    "\n",
    "# Fit using the explicit (summed, squared) distance function D and minimize\n",
    "result = spop.minimize(D, x0=[-1,1])\n",
    "print(f'Result of minimize : f(x) = exp({result.x[0]:.2f} x) sin({result.x[1]:.2f} x)')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x_i, y_i, 'ks')\n",
    "plt.plot(x, f(x, result.x))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà, on a fitté une fonction _à la main_ ! On peut aller _un peu plus rapidement_ sous Scipy en ne définissant pas explicitement $D$ (la somme des carrés des distances) mais simplement les distances individuelles $d_i$ : \n",
    "\n",
    "$$ d_i = y_i - f(x_i)$$\n",
    "\n",
    "et en appelant `scipy.optimize.least_squares()`. Cette fonction renvoie _grosso modo_ le même objet que `minimize()` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "x_i = np.linspace(0,6,30)\n",
    "y_i = np.exp(-0.5*x)*np.sin(np.pi*x) + 0.2*np.random.normal(scale=0.2,size=len(x_i))\n",
    "\n",
    "def f(x, prms):\n",
    "    return np.exp(prms[0]*x)*np.sin(prms[1]*x)\n",
    "def d_i(prms):\n",
    "    return y_i - f(x_i,prms)\n",
    "\n",
    "# Fit using (non-summed, non-squared) and least_squares()\n",
    "result = spop.least_squares(d_i, x0=[-0.5,3])\n",
    "print(f'Result of least_squares : f(x) = exp({result.x[0]:.2f} x) sin({result.x[1]:.2f} x)')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x_i, y_i, 'ks')\n",
    "plt.plot(x, f(x, result.x))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajustements faciles avec `curve_fit()`\n",
    "\n",
    "Bon, maintenant que vous avez compris le concept de l'ajustement de courbes, sachez qu'il existe une fonction plus pratique sous Scipy pour fitter vos données. Elle s'appelle `scipy.optimize.curve_fit()`. Cette fois-ci, on ne définit plus ni $D$ ni $d_i$, juste $f$, et on passe à la fonction :\n",
    "\n",
    "* la fonction `f` à fitter : elle doit être écrite comme `f(x,a,b,c,...)` avec les `x` _en premier argument_, puis ensuite les __paramètres `prms` non regroupés dans une liste__. \n",
    "* les `x_i` obtenus dans l'expérience\n",
    "* les `y_i` obtenus dans l'expérience\n",
    "* un point de départ `p0` pour les différents paramètres \n",
    "\n",
    "Cette fois-ci, la fonction ne renvoie qu'un `tuple` contenant : \n",
    "* les meilleurs paramètres de _fit_ (a,b)\n",
    "* la [matrice de covariance](https://fr.wikipedia.org/wiki/Covariance). Cette matrice est de taille $N \\times N$ pour un fit à $N$ paramètres, et ses éléments diagonaux (plus exactement leur racine) nous renseigne sur les _incertitudes_ sur les paramètres de fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as spop\n",
    "\n",
    "# Data generation\n",
    "x_i = np.linspace(0,6,30)\n",
    "y_i = np.exp(-0.5*x)*np.sin(np.pi*x) + 0.2*np.random.normal(scale=0.2,size=len(x_i))\n",
    "\n",
    "def f(x, a, b):\n",
    "    return np.exp(a*x)*np.sin(b*x)\n",
    "\n",
    "# Fit using curve_fit\n",
    "ab, covariance = spop.curve_fit(f, x_i, y_i, p0=[-0.5,3])\n",
    "print(f'Result of curve_fit : f(x) = exp({ab[0]:.2f} x) sin({ab[1]:.2f} x)')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x_i, y_i, 'ks')\n",
    "plt.plot(x, f(x, ab[0], ab[1]))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pratique : \n",
    "* si vous avez beaucoup plus de points de données que de paramètres de fit, \n",
    "* et que les erreurs sur les données suivent une loi _normale_ (c'est à dire qu'il n'y a pas d'erreur systématique)\n",
    "* et que les paramètres ne sont pas trop corrélés entre eux (c'est à dire que les éléments hors-diagonale de la matrice de covariance sont faibles devant les éléments diagonaux)\n",
    "\n",
    "On peut obtenir un intervalle de confiance $I$ à partir de la valeur du paramètre estimée $p_i$ et la valeur de la matrice de covariance sur la diagonale $C_{ii}$ :\n",
    "* à 90 % : $I_i = [p_i - 1.65\\sqrt{C_{ii}} ; p_i + 1.65\\sqrt{C_{ii}}]$\n",
    "* à 95 % : $I_i = [p_i - 1.95\\sqrt{C_{ii}} ; p_i + 1.95\\sqrt{C_{ii}}]$\n",
    "* à 99 % : $I_i = [p_i - 2.58\\sqrt{C_{ii}} ; p_i + 2.58\\sqrt{C_{ii}}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = ab[0], ab[1]\n",
    "covstd_a = np.sqrt(covariance[0,0])\n",
    "covstd_b = np.sqrt(covariance[-1,1])\n",
    "\n",
    "print(f'a : 95% confidence uncertainty [{a - 1.95*covstd_a:.3f};{a + 1.95*covstd_a:.3f}]') \n",
    "print(f'b : 95% confidence uncertainty [{b - 1.95*covstd_b:.3f};{b + 1.95*covstd_b:.3f}]') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "\n",
    "### Resolution numerique d'equations differentielles \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb9c252e610b593a3421828f92740a33c32c552b9658c846af1824aa6c7c0cc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
